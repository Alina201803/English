

##11. web scraping

**源代码**

```python
import webbrowser
webbrowser.open('http://inventwithpython.com')
mapit 870 Valencia St, San Francisco, CA 94110

#!python 3
# mapIt.py - launches a map in the browser using an address from the 
# command line or clipboard.

import webbrowser, sys
if len(sys.argv) > 1:
    #Get address from command line.
    address = ' '.join(sys.argv[1:])
else:
    #Get address from clipboard.
    address = pyperclip.paste()

webbrowser.open('http://www.google.com/maps/place' + address)
```

**今日所学**

project：maplt.py with the webbrowser module

step1:figure out the URL

step2:handle the command line arguments

step3:handle the clipboard content and launch the browser



2018-09-02

**源代码**

```python
In [1]: import requests

In [2]: res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt
   ...: ')

In [3]:

In [3]: type(res)
Out[3]: requests.models.Response

In [4]: res.status_code == requests.codes.ok
Out[4]: True

In [5]: len(res.text)
Out[5]: 178981

In [6]: print(res.text[:250])
The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever.  You may copy it, give it away or
re-use it under the terms of the Proje

In [7]: res = requests.get('http://inventwithpython.com/page_that_does_not_exis
   ...: t')

In [8]: res.raise_for_status()
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-8-6ec835a6a98f> in <module>()
----> 1 res.raise_for_status()

D:\Anaconda\lib\site-packages\requests\models.py in raise_for_status(self)
    860
    861         if http_error_msg:
--> 862             raise HTTPError(http_error_msg, response=self)
    863
    864     def close(self):

HTTPError: 404 Client Error: Not Found for url: http://inventwithpython.com/page
_that_does_not_exist

In [9]: quit()

import requests
res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
try:
    res.raise_for_status()
except Exception as exc:
    print('There was a problem: %s' %(exc))
    
```

**今日所学**

downloading files from the web with the requests ,module

downloading a web page with the requests.get() function

checking for errors



2018-09-03

**源代码**

```python
import requests
res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt')
res.raise_for_status()
playFile = open('RomeoAndJuliet.txt','wb')
for chunk in res.iter_content(100000):
    playFile.write(chunk)
playFile.close()
```

**今日所学**

saving downloaded files to the hard drive

 

2018-09-09

**源代码**

```python
<strong>hello</strong> world!
Al's free <a href="http:inventwithpython.com">Python books</a>

import requests, bs4
res = requests.get('http://nostarch.com')
res.raise_for_status()
noStarchSoup = bs4.Beau
Out[17]: list

In [18]:  type(elems[0])
    ...:
Out[18]: bs4.element.Tag

In [19]: elems[0].getText()
Out[19]: 'Al Sweigart'

In [20]: str(elems[0])
Out[20]: '<span id="author">Al Sweigart</span>'

In [21]: elems[0].attrs
Out[21]: {'id': 'author'}



In [29]: import bs4
In [31]: soup = bs4.BeautifulSoup(open('example.html'), 'lxml')

In [32]: spanElem = soup.select('span')[0]

In [33]: str(spanElem)
Out[33]: '<span id="author">Al Sweigart</span>'

In [34]: spanElem.get('id')
Out[34]: 'author'

In [35]: spanElem.get('some_nonexistent_addr') == None
Out[35]: True

In [36]: spanElem.attrs
Out[36]: {'id': 'author'}
    
#! python3
# lucky.py - Opens several Google search retifulSoup(res.text,  "lxml")
type(noStarchSoup)

In [1]: import bs4

In [2]: exampleFile = open('example.html')

In [3]: exampleSoup = bs4.BeautifulSoup(exampleFile, 'lxml')

In [4]: type(exampleSoup)
Out[4]: bs4.BeautifulSoup

In [17]:  import bs4
    ...:  exampleFile = open('example.html')
    ...:  exampleSoup = bs4.BeautifulSoup(exampleFile.read(), 'lxml')
    ...: elems = exampleSoup.select('#author')
    ...: type(elems)
    ...:sults.

import requests, sys, webbrowser, bs4

print('Googling...') #display text while downlosding the Google page
res = requests.get('http://google.com/serach?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()

# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)

# Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5, len(linkElems))
for i in range(numOpen):
    webbrower.open('http://google.com' + linkElems[i].get('href'))
#-*-coding:utf-8-*-
#! python3
# downloadXkcd.py - Downloads every single XKCD cominc.
 
 
import requests,os,bs4
 
url = 'http://xkcd.com'               # starting url
os.makedirs('xkcd',exist_ok=True)      # store comics in ./xkcd
while not url.endswith('#'):
    # Download the page
    print('Downloading page %s...'% url)
    res = requests.get(url)
    res.raise_for_status()
    soup = bs4.BeautifulSoup(res.text,"html.parser")
    # Find the URL of the comic image.
    comicElem = soup.select('#comic img')
    if comicElem == []:
        print('Could not find comic image.')
    else:
        comicUrl = 'http:' + comicElem[0].get('src')
        # Download the image.
        print('Downloading image %s...'% (comicUrl))
        res = requests.get(comicUrl)
        res.raise_for_status()
        # Save the image to ./xkcd.
        imageFile = open(os.path.join('xkcd',os.path.basename(comicUrl)),'wb')
        for chunk in res.iter_content(100000):
            imageFile.write(chunk)
        imageFile.close()
     # Get the Prev button's url.
    prevLink = soup.select('a[rel="prev"]')[0]
    url = 'http://xkcd.com' + prevLink.get('href')
 
print('Done.')
```

**错题集**

```python
书上关于Project :downloading all xkcd comics 的代码会报错
Downloading page http://xkcd.com...
Downloading image //imgs.xkcd.com/comics/boathouses_and_houseboats.png...
Traceback (most recent call last):
  File "2018-09-09-1.py", line 25, in <module>
    res = requests.get(comicUrl)
  File "D:\Anaconda\lib\site-packages\requests\api.py", line 70, in get
    return request('get', url, params=params, **kwargs)
  File "D:\Anaconda\lib\site-packages\requests\api.py", line 56, in request
    return session.request(method=method, url=url, **kwargs)
  File "D:\Anaconda\lib\site-packages\requests\sessions.py", line 461, in reques                                                                                                                t
    prep = self.prepare_request(req)
  File "D:\Anaconda\lib\site-packages\requests\sessions.py", line 394, in prepar                                                                                                                e_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "D:\Anaconda\lib\site-packages\requests\models.py", line 294, in prepare
    self.prepare_url(url, params)
  File "D:\Anaconda\lib\site-packages\requests\models.py", line 354, in prepare_                                                                                                                url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL '//imgs.xkcd.com/comics/boathouse                                                                                                                s_and_houseboats.png': No schema supplied. Perhaps you meant http:////imgs.xkcd.                                                                                                                com/comics/boathouses_and_houseboats.png?
、
soup = bs4.BeautifulSoup(res.text,"html.parser")
加上"html.patser"后 问题解决 
下载的图片实在太多了，手动停掉，感觉到了电脑做事的方便。

html.parser — Simple HTML and XHTML parser 
https://docs.python.org/3/library/html.parser.html

```

**今日所学**

HTML

resourses for learning HTML

viewing the source html of a web page

opening your broeser's developer tools

using the developer tools to find html elements

parsing html with beautifulsoup module

project "i'm feeling lucky " google search

project downloading all xkcd comics



